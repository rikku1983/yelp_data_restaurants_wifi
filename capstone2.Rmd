---
title: "STAT 5371 final project----WiFi matters?"
author: "Li Sun"
date: "November 19, 2015"
output: pdf_document
---
# Introduction:
I tried to find out, should restaurants' owners provide wifi (free or paid) to customers to make them feel better. Part I we finished converting 3 json files into a single data frame containing most relevant information. 

# Methods and Data:
Data source are from 'http://www.yelp.com/dataset_challenge'. Data comes in json format, and there are 5 files: user, business, checkin, tip, and reviews. To answer my specific question: 'Does wifi access really helps improve customers' experience in restuarants?' I choose only three data sets from these five, user, business, and reviews. And I used regression method to study effects of different variables on response, stars. I want to see if free wifi can have positive effects to stars of that business or is just irrelevant, in a good model considering all relevant variables

```{r, echo=FALSE, message=FALSE}
# library(jsonlite)
# library(tm)
# library(ngram)
# library(stringr)
library(ggplot2)
library(wordcloud)
# library(RColorBrewer)
library(gplots)
library(car)
library(lars)
library(glmnet)
busi3 <- readRDS("busi3.rds")
```
## 1. Missing values and data types
First thing is get rid of missing values. I didnt impute missing values in this case because most of the missingness occur in factor like variables, which could not to be imputed easily. Also some of the columns are in data types which are not good for later analysis like "array", "character". We will convert them to "numeric" and "factor". Identity columns are separated to a new data frame "busi id df". Finally, we also remove those variables with very low variance, which means over 99% of all observations of those columns are unique value. Because the coeficients of those variables are going to be very large.

```{r, echo=FALSE}
#convert class array to numeric
arrayidx <- which(sapply(busi3, class)=="array")
for(i in arrayidx){
  busi3[,i] <- as.numeric(busi3[,i])
}
#remove original categories var, and change credit cards var to logic
busi3 <- busi3[,-3]
var9<-unlist(as.character(busi3[,9]))
var9[var9=="TRUE"] <- 1
var9[var9=="FALSE"] <- 0
var9[var9=="NULL"] <- NA
busi3[,9] <- as.numeric(var9)
#Check sparcity of columns 
sparcity <- sapply(busi3, function(x){sum(is.na(x))/length(x)})
par(mfrow=c(1,1),mar=c(14,4,4,2))
# barplot(sort(sparcity, decreasing = T)[1:50], las=2, cex.names=0.8, ylab="sparsicity")
# abline(h=0.1,col=4,lty=2)
#remove cols with sparcity > 0.1
busi4 <- busi3[,sparcity < 0.1]
#seperate identity columns: business_id, name
busi_id_df <- busi4[,c(1,5)]
busi4 <- busi4[,-c(1,5)]
sum(complete.cases(busi4))
# maintain only completcases
busi4 <- busi4[complete.cases(busi4),]
busi_id_df <- busi_id_df[complete.cases(busi4),]
saveRDS(busi_id_df, "busi_id_df.rds")
#change all cols to classes as factors or numeric, easy to analyze
unique(sapply(busi4, class)[1:50])
busi4$city <- as.factor(busi4$city)
busi4$attributes.Alcohol <- as.factor(busi4$attributes.Alcohol)
busi4$`attributes.Noise Level` <- as.factor(busi4$`attributes.Noise Level`)
busi4$attributes.Attire <- as.factor(busi4$attributes.Attire)
busi4$attributes.wifi <- as.factor(busi4$attributes.wifi)
#2 variables contain only one unique value, so we will get rid of them
busi4 <- busi4[, -c(1, 5, 51)]
div=numeric()
for(i in 1:ncol(busi4)){
  div<- c(div, sort(table(busi4[,i]), decreasing=T)[1]/nrow(busi4))
}
x <- seq(0.95,1,by=0.001)
y <- numeric()
for(i in seq_along(x)){
  y <- c(y, sum(div < x[i]))
}
par(mfrow=c(1,1),mar=c(6,4,4,2))
#plot(x, y, pch=19, xlab ="percentage of most frequent value", ylab ="number of var with less uniform data")
df <- busi4[,div <0.99]
saveRDS(df, "df_for_EDA.rds")
```

##EDA
###What people say about wifi
```{r, echo=FALSE, results='hide', message=FALSE}
df0 <- readRDS("yelp_tm.rds")
wifidf <- df0[df0$wifi_sen!="",]
wifidf$date <- strptime(wifidf$date, "%Y-%m-%d")
df0$date <- strptime(df0$date, "%Y-%m-%d")
nwifirev_time <- tapply(rep(1, nrow(wifidf)), format(wifidf$date, "%Y"), sum)
nrev_time <- tapply(rep(1, nrow(df0)), format(df0$date, "%Y"), sum)[-c(1,2)]
carewifi <- nwifirev_time/nrev_time
plotdata<- cbind(nwifirev_time, nrev_time, carewifi)
saveRDS(plotdata, "plotdata")

#png(file="1.png",width=2500,height=1700,res=350)
qplot(names(nwifirev_time), carewifi, geom="bar",stat="identity", xlab="", ylab="percent of reviews mentioned wifi", fill=nwifirev_time) + geom_text(label=nwifirev_time, vjust=-0.5) + ylim(0, 0.006)
#dev.off()

stem_wifi_text<- readRDS("stem_wifi_text.rds")
find_2_gram <- function(vec){
  if(length(vec)<2){return("")}
  v1 <- vec[1:length(vec)-2+1]
  v2 <- vec[2:length(vec)]
  g2<-paste(v1, v2)
  g2
}
gram2<-list()
for(i in seq(stem_wifi_text)){
  if(length(stem_wifi_text[[i]])<2){}
  gram2[[i]] <- find_2_gram(stem_wifi_text[[i]])
}
#build word could from input of a list of documents
mywc <- function(doclist){
  freqdf<-as.data.frame(table(unlist(doclist)),stringsAsFactors=F) 
  wordcloud(freqdf$Var1, freqdf$Freq, max.words=50, colors=brewer.pal(8, "Dark2"))
}
```
```{r, fig.width=6, fig.height=6}
#png(file="2.png",width=2500,height=1700,res=350)
mywc(stem_wifi_text)
#dev.off()
#png(file="3.png",width=2500,height=1700,res=350)
mywc(gram2)
#dev.off()
```

###City and category reduction
First we found the cities var contains too many values, and not distributed uniformly. So in order to increase the efficincy of regression, lower the number of dummy variables in the final model, we will group some of the citis 
```{r, echo=FALSE, results='hide', message=FALSE}
df <- readRDS("df_for_EDA.rds")
unique(df$city)
# tapply()
city<-names(sort(table(df$city), decreasing = T)[1:4])
# we will leave first 5 cities and combine all left as "others"
oricity <- as.character(df$city)
for(i in 1:nrow(df)){
  if(oricity[i] %in% city){next}
  else{oricity[i]<-"others"}
}
df$city <- as.factor(oricity)
df$attributes.wifi <- relevel(df$attributes.wifi, "no")
```
We still have too many variables specifying types of restuarants. Some of the types are rare, with less occurance, that would be of our less interest. So I want to cut the type of restuarants down to several majors types. The standard of maintainning certain types depends on occurance and also we want to cover almost all the business in list.

```{r, echo=FALSE, results='hide', message=FALSE}
saveRDS(df[,43:78], "categMatrix.rds")
occr <- sort(apply(df[,43:78], 2, sum), decreasing = T)
occr
covr <- numeric()
temp <- rep(0, nrow(df))
for(i in 1:length(occr)){
  temp <- temp + df[,names(occr)[i]]
  covr <- c(covr, sum(temp!=0)/nrow(df))
}
```
Based on this figuer, we will choose first 13 types which cover `r covr[13]` of all observations. And store full categories in categ data frame

```{r echo = FALSE, fig.width=4, fig.height=4}
#png(file="4.png",width=2500,height=1700,res=350)
qplot(x=1:length(occr), y=covr, size=occr, colour=occr, geom="point", xlab="number of types", ylab="coverage of all observations") + geom_vline(xintercept = 13)
#dev.off()

df1 <- cbind(df[, 1:42], df[, names(occr)[1:13]])
categ <- df[,43:78]
```

## Check the correlations
Highly correlated predictor can cause problems, so lets check the correlations

```{r echo = FALSE, fig.width=4, fig.height=4}
corm <- cor(as.matrix(df1[,-c(1, 9, 10,12, 17)]))
saveRDS(corm, "corm.rds")
png(file="5.png",width=2500,height=1700,res=350)
heatmap.2(corm, cexRow = 0.4, cexCol=0.4, density.info = "none")
dev.off()
```

This tell us:
1. stars which is from business table are highly correlated to average stars we want to analyze, if we include this in the model, we will get model with high R square but can provide no information about what people really like. So I will exclude this business star from our model
2. two variables are almost the same, "review_count", and "nrev", they are talking about the same thing, so I will remove "review count" . Similarly, "nightlife" and "bars" overlap significantly, so remove "nightlife"

```{r, echo=FALSE, results='hide', message=FALSE}
#remove "review_count", "stars", "Nightlife"
df2 <- df1[, -c(2,3, 47)]
saveRDS(df2, "df2.rds")
```
Finally, lets look at some numeric variables distributions

```{r, echo=FALSE, result = 'hide' , message=FALSE}
df2<-readRDS("df2.rds")
#png(file="6.png",width=2500,height=1700,res=350)
par(mfrow=c(2,2))
par(mar=c(2,2,2,2))
hist(df2$ave_star)
hist(df2$ave_pos)
hist(df2$ave_neg)
hist(df2$ave_tok)
#dev.off()
hist(log(df2$nrev_wifi[df2$nrev_wifi!=0]))
hist(df2$sentiment)
```
Transform skewed the data to lower the skewness

```{r, echo=FALSE, results='hide', message=FALSE}
df2$ave_star <- 1.5^df2$ave_star
names(df2)[35] <- "exp_ave_star"
#df2$nrev_wifi[df2$nrev_wifi!=0] <- log(df2$nrev_wifi[df2$nrev_wifi!=0])
names(df2)[39] <- "log_nrev_wifi"
```


# Exploratory modeling
## full model
Let's throw all variables into the regular linear model and see how this full model perform and compare it to weighted model.

```{r, echo=FALSE, results='hide', message=FALSE}
lmsum <- function(fit){
  return(c("Rsqr"=summary(fit)$adj.r.squared, "sigma" = sigmaHat(fit),"vif" = sort(vif(fit)[,1], decreasing=T)[1:5], fit$coefficients["attributes.wififree"], fit$coefficients["attributes.wifipaid"]))
}

f <- lm(exp_ave_star~., data=df2)
lmsum(f)
summarym <- lmsum(f)
```
Compare to weighted lm model: weighted model

```{r echo=FALSE, results='hide', message=FALSE}
wf <- lm(exp_ave_star~., data=df2, weights = sqrt(nrev))
lmsum(wf)
summarym <- rbind(summarym, lmsum(wf))
```
From here, we know we need to use weighted model because weighted model give us much higher R square. 
In both models we see four variables with pretty high vif indicating multicolinearity. To deal with this, first let's try to center numeric vaiables to reduce multicolinearity
```{r, echo=FALSE, results='hide', message=FALSE}
df2c <- df2
df2c[,35:40] <- scale(df2c[,35:40])
fc <- lm(exp_ave_star~., data=df2c)
lmsum(fc)
wfc <- lm(exp_ave_star~., data=df2c, weights = sqrt(nrev))
lmsum(wfc)
summarym <- rbind(summarym, lmsum(fc), lmsum(wfc))
```
Above all, 
```{r, echo=FALSE, message=FALSE}
saveRDS(summarym, "summarym.rds")
summarym
```
1. Center numeric data is barely improving anything.
2. Weighted model significantly increase adjusted R square but also increase the vif.
Considering even without weights, the vif of the top 4 are so high that we have to remove one or several. So we will stick to weighted and non-centered model from now on.
We can either remove variable "sentiment" or both "ave pos" and "ave neg". Let's try both

```{r, echo=FALSE, results='hide', message=FALSE}
df3 <- df2[,-40]
f3 <- lm(exp_ave_star~., data=df3, weights= sqrt(nrev))
lmsum(f3)

df4 <- df2[,-c(36,37)]
f4 <- lm(exp_ave_star~., data=df4, weights= sqrt(nrev))
lmsum(f4)

summarym <- rbind(summarym, lmsum(f3), lmsum(f4))
rownames(summarym) <- c("original", "weighted", "centered", "centered_weighted", "-sentiment", "-pos and -neg")
saveRDS(summarym, "summarym.rds")
write.csv(summarym, "summarym.csv")
```
From here we find when we remove  "ave pos" and "ave neg", full model has higher R squared and smaller maximum vif. So we will do so from now on. 

```{r, echo=FALSE, results='hide', message=FALSE}
names(df4) <- gsub("[ -]", "_", names(df4))
```
## Do we have interactions here?
We have lots of categorical data, so a natural question to ask is do any of those interact. 

```{r, echo=FALSE, message=FALSE, cache=TRUE}
#Interaction among all possible variables
gf <- lm(exp_ave_star~.*., data=df4, weights= sqrt(nrev))
#data.frame(summary(gf)$coef[summary(gf)$coef[,4] <= .001, 4])
#png(file="7.png",width=2500,height=1700,res=350)
coplot(exp_ave_star~sentiment|ave_tok, data=df4)
#dev.off()
# coplot(exp_ave_star~sentiment|as.factor(attributes.Parking.lot), data=df4)
# Interaction between wifi and other variables
gfwifi <- lm(exp_ave_star~attributes.wifi*., data=df4, weights = sqrt(nrev))
summary(gfwifi)
#data.frame(summary(gfwifi)$coef[summary(gfwifi)$coef[,4] <= .001, 4])
#coplot(exp_ave_star~nrev|as.factor(attributes.wifi), data=df4)

#png(file="8.png",width=2500,height=1700,res=350)
coplot(exp_ave_star~log_nrev_wifi|as.factor(attributes.wifi), data=df4)
#dev.off()

#coplot(exp_ave_star~log_nrev_wifi|nrev, data=df4)
nrev_wifi_p <- df4$log_nrev_wifi/df4$nrev
#coplot(exp_ave_star~nrev_wifi_p|as.factor(attributes.wifi), data=df4)
```
We do find a little interaction between 
1. number of reviews and number of reviews talking about wifi with wifi status
2. ave_tok and sentiment
So we will include those 3 interacting term in my model by add interacting variables in the dataset.

```{r echo=FALSE, results='hide', message=FALSE}
df5 <- df4
df5$int_tok_sent <- df5$ave_tok * df5$sentiment
df5$int_n_nowifi <- df5$log_nrev_wifi * (df5$attributes.wifi=="no")
df5$int_n_freewifi <- df5$log_nrev_wifi * (df5$attributes.wifi=="free")
df5$int_n_paidwifi <- df5$log_nrev_wifi * (df5$attributes.wifi=="paid")
# to maintain low correlation, remove original log_nrev_wifi col
df5 <- df5[,-37]
f5 <- lm(exp_ave_star~., weights=sqrt(nrev), data=df5)
lmsum(f5)
summarym <- rbind(summarym, lmsum(f5))
colnames(summarym)[3:7] <-c("vif") 
rownames(summarym)[7] <- "+interaction1"
#remove "int_tok_sent"
df6<- df5[,-50]
f6<- lm(exp_ave_star~., weights=sqrt(nrev), data=df6)
lmsum(f6)
summarym <- rbind(summarym, lmsum(f6))
rownames(summarym)[8] <- "+interaction2"
saveRDS(df6, "df6.rds")
```
Comparing all full models

```{r}
summarym
write.csv(summarym, "summarym.csv")
```

## Model selection
After we have our full model, several methods are used to choose best parsimonious model by lasso

```{r, echo=FALSE, results='hide', message=FALSE, fig_width= 3, fig_height= 3}
#Construct design matrix
df6 <- readRDS("df6.rds")
xx <- df6[,-35]
xx[,7]  <- relevel(xx[,7], "none")
# Cast multilevel variable into design matrix with dummy variable
m1 <- model.matrix(~xx[,1]-1)[,-1]
m7 <- model.matrix(~xx[,7]-1)[,-1]
m8 <- model.matrix(~xx[,8]-1)[,-1]
m10 <- model.matrix(~xx[,10]-1)[,-1]
m15 <- model.matrix(~xx[,15]-1)[,-1]
xx2 <- cbind(m1, xx[,2:6], m7, m8, xx[,9], m10, xx[,11:14], m15, xx[,16:ncol(xx)])
colnames(xx2)[15] <- "attributes.Has_TV"
colnames(xx2) <- gsub("xx\\[, [0-9]+\\]", "", colnames(xx2))
colnames(xx2) <- gsub("attributes.", "", colnames(xx2))
for(i in ncol(xx2)){
  xx2[,i] <- xx2[,i] * 1
}
xx2 <- as.matrix(xx2)
yy <- df6[,35]
las <- glmnet(xx2, yy, weights = sqrt(df6$nrev))
par(mfrow=c(1,1))
par(mar=c(5,5,4,2))
#png(file="9.png",width=2500,height=1700,res=350)
plot(las)
#dev.off()
print(las)
coef(las, 0.6)
cvlas <- cv.glmnet(xx2, yy, weights = sqrt(df6$nrev), nfolds= 10)
#png(file="10.png",width=2500,height=1700,res=350)
plot(cvlas)
#dev.off()

```
This tell us:
To explain number of stars, variable sentiment almost explained all of it. And all other variables combined have only marginal effects after we include sentiment in the model. And also sentiment of customers reflect overall experience to that restuarant, which means all the attributes we gave them are likely actually contributing to sentiment and then to stars.
Our focus is about wifi access. So let's leave R square aside and try model without sentiment.

```{r, echo=FALSE, results='hide', message=FALSE, fig_width= 3, fig_height= 3}
xx3 <- xx2[,-44]
las3 <- glmnet(xx3, yy, weights = sqrt(df6$nrev))
#png(file="11.png",width=2500,height=1700,res=350)
plot(las3)
#dev.off()
print(las3)
cvlas3 <- cv.glmnet(xx3, yy, weights = sqrt(df6$nrev), nfolds= 10)
cvlas3$lambda.min

#png(file="12.png",width=2500,height=1700,res=350)
plot(cvlas3)
#dev.off()

coef(las3, cvlas3$lambda.1se)
#plot coeficients
coef3<-coef(cvlas3, s = "lambda.1se")
paranames <- c("intercept", colnames(xx3))[as.matrix(coef3) != 0]
coef3 <- as.numeric(coef3)[as.matrix(coef3) != 0]

ord <- order(coef3, decreasing=T)
coef3 <- coef3[ord]
paranames <- paranames[ord]

#Print coef
t(data.frame("variables"=paranames, "values"=coef3))

#Get rid of intercetp 
coef3 <- coef3[-1]
paranames2 <- paranames[-1]
coefgp <- as.factor((coef3>0) * 1 + (coef3<0)*2)

#saveRDS(data.frame("Chosen_Var"=factor(paranames2, levels=paranames2), "Fit_Value"=coef3), "coefplotdata.rds")
#Plot coeficients
```

```{r, fig_width= 6, fig_height= 6}
#png(file="12.png",width=2500,height=1700,res=200)
suppressWarnings(ggplot(data=data.frame("Chosen_Var"=factor(paranames2, levels=paranames2), "Fit_Value"=coef3), aes(Chosen_Var, weight=Fit_Value)) +geom_bar(aes(fill=coefgp)) + labs(title = "Coeficients of Lasso Regression Model Fit", x="Words/Variables/Features", y="Coefficients") + theme(axis.ticks = element_blank(), axis.text.x = element_blank()) + geom_text(aes(y=c(rep(-0.5,sum(coef3 >0)), rep(0.5,sum(coef3 <0))), label=paranames2, angle=-90)) + ylim(-1.7, 1.5))
#dev.off()
```
#Results:
In this data set, variable sentiment is too good to explain the ratings been give. And this sentiment are also related to all other variables to a certain extend. So makes all the coeficients of others not so significant. If we exclude sentiment and just analyze the contributions of other attributes. R square dropped dramatically but there are still 20% been accounted. 
When we include sentiment, coefficient of wifi free is always below zero. Which can be explained by multicolinearity. When sentiment excluded, the coefficient of wifi free become positive and wifi paid remaind negative. And both are significant.

#Discussion:
Wifi are not as important as I think. People most likely will not give higher stars because of free wifi. However, paid wifi is hurting people's feeling from various evidence. So if you want to have your own restuarant, please provide free wifi or don't provide wifi access. One reason that wifi is irrelevant might be the wireless data is getting cheaper. Because less and less people mention wifi along the past 5 or 6 years.
BIG BIG confounder here!! Nothing about service and food quality considered, which cannot be easily quantified unless extracting from review. 
Lots of work could be done to do more text mining and also we can study which type of restuarants needs free wifi than others.

Thank you for reading!!
```{r, echo = FALSE}
# fooddf <- df6[df6$Food==1,]
# foodf <- lm(exp_ave_star~., data=fooddf, weights = sqrt(nrev))
# summary(foodf)
# fooddf2 <- fooddf[, -37]
# foodf2 <- lm(exp_ave_star~., data=fooddf2, weights = sqrt(nrev))
# summary(foodf2)
# 
# nrev_wifi
```

