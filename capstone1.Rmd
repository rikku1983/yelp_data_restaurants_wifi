---
title: "capstone part 1----WiFi matters?"
author: "Li Sun"
date: "November 19, 2015"
output: html_document
---

#Getting Data
Data source are from 'http://www.yelp.com/dataset_challenge'. Data comes in json format, and there are 5 files: user, business, checkin, tip, and reviews. To answer my specific question: 'Does wifi access really helps improve customers' experience in restuarants?' I choose only three data sets from these five, user, business, and reviews.

Data are read in by jsonlite package in r and stored as RDS file for faster loading in future.
```{r}
#load jsonlite package
library(jsonlite)
library(tm)
# library(ngram)
# library(stringr)
library(ggplot2)
library(wordcloud)
# library(RColorBrewer)
# library(gplots)

#read business file
file.path = file.choose()
business<-stream_in(file(file.path))
saveRDS(business, "business.rds")

#read review file
file.path = file.choose()
review<-stream_in(file(file.path))
saveRDS(review, "review.rds")

#read user file
file.path = file.choose()
business<-stream_in(file(file.path))
saveRDS(user, "user.rds")

#Read files back
business <- readRDS("business.rds")
review <- readRDS("review.rds")
user <- readRDS("user.rds")
```

#Preprocess data to one single data.frame
##flat all data frames
```{r}
fbusi <- flatten(business)
frev <- flatten(review)
fusr <- flatten(user)
```
##Subsetting and merging
What we are doing here:
1. only extract data for restuarants.
2. get rid of obviously unrelated columns in business data which are not relevant to restuarants.
3. for Geometric data which might be useful later, we stored them in a separate data frame "geo".
4. get only restuarants which are open and wifi data are not NA.
5. merging the 3 data: user, business and review.

```{r}
#for business dataset
#how many different type of services
unique(unlist(fbusi$categories))
mycat <- c("Restaurants")
#We don't need full_address(2), neighborhoods(8), longitude(9), state(10), latitude(12), hours(14:27), "attributes.By Appointment Only"(28), [57] "attributes.Accepts Insurance" , [86]"attributes.Hair Types Specialized In.coloring" [87] "attributes.Hair Types Specialized In.africanamerican" "attributes.Hair Types Specialized In.curly" [89] "attributes.Hair Types Specialized In.perms"           "attributes.Hair Types Specialized In.kids"  [91] "attributes.Hair Types Specialized In.extensions"      "attributes.Hair Types Specialized In.asian" [93] "attributes.Hair Types Specialized In.straightperms"
#We need "open" = TRUE, business category falls into mycat, and not "NA" on attributes.wife
names(fbusi)[46] <- "attributes.wifi"
geo <- fbusi[,c(1,2,5,7,8,9,10,12)]
saveRDS(geo, "geo.rds")
rowidx <- fbusi$open & sapply(fbusi$categories, function(x) length(intersect(mycat, x))!=0) & !is.na(fbusi$attributes.wifi)
fbusi <- fbusi[rowidx, -c(2,8:10, 12, 14:28, 57, 86:93)]
#merge business and review tables
df <- merge(frev, fbusi, by="business_id")
df <- df[,-c(7,17)]
names(df)[4] <- "stars"
names(df)[15] <- "busi_stars"
#merge df and user tables
df0 <- merge(df, fusr, by="user_id", all.x = TRUE)
```
##Text quantification
Let's do some text processing to convert text information to some simple numeric infomation. So I can use them in regression or other model later.
### Do people talk about wifi?
Here we extract sentences in the reviews containing "wifi" or "internet". And put the sentences in to a additional column "wifi_sen"
```{r}
#Function to extract a sentence containing certain words
findSentence <- function(oristr, word){
  x <- strsplit(oristr, split = "[\\.\\!\\?\n]")[[1]]
  idx <- grep(word, x, ignore.case = T)
  return(toString(x[idx]))
}
df0$wifi_sen <- rep("", nrow(df0))
wifiidx <- grep("wi-?fi", df0$text, ignore.case=T) 
for(i in wifiidx){
  df0$wifi_sen[i] <- c(findSentence(df0$text[i], "wi-?fi"))
}
internetidx <- grep("internet", df0$text, ignore.case = T)
for(i in internetidx){
  if(df0$wifi_sen[i]==""){df0$wifi_sen[i] <- findSentence(df0$text[i], "internet")}
  else{df0$wifi_sen[i] <- paste(df0$wifi_sen[i], findSentence(df0$text[i], "internet"), sep=".")}
}
#idx1 <- unique(c(wifiidx, internetidx))
```
### Extract 3 things from all reviews
Extract 3 things from each reviews and store them in new columns:
  1. Total words
  2. Positive words
  3. Negative words
Before that, I have to convert all letters to lower case, remove stopwords, remove puctuations.

```{r}
myStopwords <- c(stopwords('english'), "if")
positive_words <- readLines("http://www.idiap.ch/~apbelis/hlt-course/positive-words.txt")[-c(1:35)]
negtive_words <- readLines("http://r.chrisrooney.co.uk/presentation/data/negative-words.txt")[-c(1:35)]

#Function to count positive words
pos_count <- function(char, pos = positive_words){
  sum(strsplit(char, split=" ")[[1]] %in% pos)
}
#Function to count negative words
neg_count <- function(char, neg = negtive_words){
  sum(strsplit(char, split=" ")[[1]] %in% neg)
}

df0$text2 <- rep("", nrow(df0))
df0$token_num <- rep(0, nrow(df0))
df0$pos_num <- rep(0, nrow(df0))
df0$neg_num <- rep(0, nrow(df0))
for(i in 1:nrow(df0)){
  txt <- df0$text[i]
  txt <- tolower(txt)
  txt <- removeWords(txt, myStopwords)
  txt <- removePunctuation(txt)
  txt <- removeNumbers(txt)
  #remove extra spaces
  txt <- stripWhitespace(txt)
  if(substr(txt,1,1)==" "){txt <- gsub("^ ", "", txt)}
  # stem words
  df0$text2[i] <- txt
  # df0$stemedtext[i] <- strsplit(txt, split = " ")[[1]]
  # dict <- unique(c(dict, strsplit(txt, split = " ")[[1]]))
  # stemWords <- stemDocument(oriWords)
  # stemedtext[[i]] <- stemWords
  df0$token_num[i] <- length(strsplit(txt, split=" ")[[1]])
  df0$pos_num[i] <- pos_count(txt)
  df0$neg_num[i] <- neg_count(txt)
}
saveRDS(df0, "yelp_tm.rds")
```

## Do people care about wifi? What pepole say about wifi?
We subset all rows where people mentioned about "wifi" or "internet", it will be interesting to see what people say about wifi
```{r}
df0 <- readRDS("yelp_tm.rds")
wifidf <- df0[df0$wifi_sen!="",]
wifidf$date <- strptime(wifidf$date, "%Y-%m-%d")
df0$date <- strptime(df0$date, "%Y-%m-%d")
nwifirev_time <- tapply(rep(1, nrow(wifidf)), format(wifidf$date, "%Y"), sum)
nrev_time <- tapply(rep(1, nrow(df0)), format(df0$date, "%Y"), sum)[-c(1,2)]
carewifi <- nwifirev_time/nrev_time
qplot(names(nwifirev_time), carewifi, geom="bar",stat="identity" , xlab="", ylab="percent of reviews mentioned wifi", fill=nwifirev_time) + geom_text(label=nwifirev_time, vjust=-0.5)

saveRDS(wifidf, "wifidf.rds")
wifi_text <- wifidf$wifi_sen
dict<-as.character()
myStopwords <- c(stopwords('english'), "if")
for(i in seq(wifi_text)){
  wifi_text[i] <- gsub("wi-?fi", "wifi", wifi_text[i], ignore.case = T)
  wifi_text[i] <- gsub("internet", "wifi", wifi_text[i], ignore.case = T)
  wifi_text[i] <- tolower(wifi_text[i])
  wifi_text[i] <- removeWords(wifi_text[i], myStopwords)
  wifi_text[i] <- removePunctuation(wifi_text[i])
  wifi_text[i] <- removeNumbers(wifi_text[i])
  wifi_text[i] <- stripWhitespace(wifi_text[i])
  if(substr(wifi_text[i],1,1)==" "){wifi_text[i] <- gsub("^ ", "", wifi_text[i])}
  dict <- c(dict, strsplit(wifi_text[i], split = " ")[[1]])
}
#Stem words
stem_wifi_text <- list()
for(i in 1:length(wifi_text)){
  oriwords <- strsplit(wifi_text[i], split=" ")[[1]]
  stemedwords <- stemDocument(oriwords)
  newwords <- stemCompletion(stemedwords, dictionary = dict, type = "prevalent")
  stem_wifi_text[[i]] <- newwords
}

saveRDS(stem_wifi_text, "stem_wifi_text.rds")
#Find 2-grams
find_2_gram <- function(vec){
  if(length(vec)<2){return("")}
  v1 <- vec[1:length(vec)-2+1]
  v2 <- vec[2:length(vec)]
  g2<-paste(v1, v2)
  g2
}
gram2<-list()
for(i in seq(stem_wifi_text)){
  if(length(stem_wifi_text[[i]])<2){}
  gram2[[i]] <- find_2_gram(stem_wifi_text[[i]])
}
#build word could from input of a list of documents
mywc <- function(doclist){
  freqdf<-as.data.frame(table(unlist(doclist)),stringsAsFactors=F) 
  wordcloud(freqdf$Var1, freqdf$Freq, max.words=66, colors=brewer.pal(8, "Dark2"))
}
mywc(stem_wifi_text)
mywc(gram2)
```

## Construct new variables will be used in future analysis from text mining
Now we are ready to get down to only businesses. Our goal here is get a tidy data frame with rows of different restuarants and columns of different variables relevant to restuarants rating and wifi. So we will wrap up information extracted from previous text mining to several new variables and merge and attach to form the data.frame
  1. number of reviews per business
  2. average review stars
  2. average postive words
  3. average negtive words
  4. average token numbers
  5. sentiment = (positive words - negtive words)/token numbers 
  6. number of reviews talking about wifi
```{r}
#number of reviews
nrev <- as.data.frame(table(df0$business_id), stringsAsFactors = F)
names(nrev) <- c("business_id", "nrev")
busi2 <- merge(fbusi, nrev, by="business_id")
#average review stars
ave_star <- tapply(df0$stars, df0$business_id, mean)
ave_stardf <- data.frame(business_id=names(ave_star), ave_star=ave_star)
busi2 <- merge(busi2, ave_stardf, by="business_id")
#average positive words
ave_pos <- tapply(df0$pos_num, df0$business_id, mean)
ave_pos <- data.frame(business_id=names(ave_pos), ave_pos=ave_pos)
busi2 <- merge(busi2, ave_pos, by="business_id")
#average negative words
ave_neg <- tapply(df0$neg_num, df0$business_id, mean)
ave_neg <- data.frame(business_id=names(ave_neg), ave_neg=ave_neg)
busi2 <- merge(busi2, ave_neg, by="business_id")
#average token numbers
ave_tok <- tapply(df0$token_num, df0$business_id, mean)
ave_tok <- data.frame(business_id=names(ave_tok), ave_tok=ave_tok)
busi2 <- merge(busi2, ave_tok, by="business_id")
#average number of reviews mentioned wifi
nrev_wifi <- as.data.frame(table(wifidf$business_id), stringsAsFactors = F)
names(nrev_wifi) <- c("business_id", "nrev_wifi")
busi2 <- merge(busi2, nrev_wifi, by="business_id", all.x=T)
busi2$nrev_wifi[is.na(busi2$nrev_wifi)] <- 0
#sentiment
busi2$sentiment <- (busi2$ave_pos - busi2$ave_neg)/busi2$ave_tok
```
##Convert the categories to categ_matrix
We want to change categories of all rows into a form easy to be analyzed
```{r}
#How many unique categories are there?
for(i in 1:nrow(fbusi)){
  fbusi$categories[[i]] <- gsub("[()]", "",  fbusi$categories[[i]])
}
categ <- fbusi$categories
uniq_categ <- unique(unlist(sapply(categ, unlist)))
uniq_categ
for(i in seq(categ)){
  categ[i] <- toString(unlist(categ[i]))
}
categ_df <- as.numeric(grepl(uniq_categ[1], categ))
for(i in 2:length(uniq_categ)){
  tempcol <- as.numeric(grepl(uniq_categ[i], categ))
  categ_df <- cbind(categ_df, tempcol)
}
colnames(categ_df) <- uniq_categ
categ_df <- as.data.frame(categ_df)
par(mar=c(12,4,4,3))
barplot(sort(sapply(categ_df[,-5], sum), decreasing = T))
# temp=rep(0, nrow(categ_df))
# i=1
# z<-numeric()
# temp <- categ_df[,1]
# while(sum(temp==0)){
#   i = i+1
#   z <- c(z,sum(temp==0))
#   temp<-temp + categ_df[,i]
#   if(i > 255){break; print("unsuccess")}
# }
categ_df <- cbind(fbusi$business_id, categ_df)
names(categ_df)[1] <- "business_id"
#merge to busi2
busi3 <- merge(busi2, categ_df, by="business_id", all.x=T)

saveRDS(busi3, "busi3.rds")
```
# Conclusion for part I
This concludes the part1 of this yelp data analysis. We end up with a very tidy data frame "busi3", with 12344 observations and 336 variables. It is ready to proceed to next stage of further clean up the data and get it ready for modeling.

